{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66fb5c9e-e6ce-494b-a571-c3875fbbdb3b",
   "metadata": {},
   "source": [
    "# 09: Create compiled datasets and summary statistics\n",
    "*Compile datasets from different GCMs and create summary statistics (e.g., annual maxima, days over threshold, heatwave days).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b142e928-11ab-4b90-97bb-159cb560c16d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import dask\n",
    "import fsspec\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xclim\n",
    "from dask.distributed import Client\n",
    "from utils import gcm_list, load_multimodel_results, summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d6583f-d175-4e0d-9560-01f71ee9a408",
   "metadata": {},
   "source": [
    "Set up cluster to handle multiprocessing using a Dask client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f6fc70-19f5-4fda-b054-205bf614c524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = Client(threads_per_worker=1, n_workers=4, silence_logs=logging.ERROR)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35fc248-ef1e-4438-8b6d-c0a4a7e73a4e",
   "metadata": {},
   "source": [
    "Compile all individual GCM datasets into one multimodel dataset that is optimally chunked for timeseries analysis. Create summaries for each analysis period and multimodel medians of those summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4ca5c-45fe-4e0a-8ef0-6ed22d6410f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rename_dict = {\"wbgt-shade\": \"scen\", \"wbgt-sun\": \"wbgt-sun\"}\n",
    "analysis_period = {\n",
    "    \"historical\": slice(\"1985\", \"2014\"),\n",
    "    \"ssp245-2030\": slice(\"2020\", \"2039\"),\n",
    "    \"ssp245-2050\": slice(\"2040\", \"2059\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f1441-892d-45b9-9d39-0eb8d5fb25e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for metric in [\"wbgt-sun\", \"wbgt-shade\"]:\n",
    "    full_ds = load_multimodel_results(gcm_list, metric)\n",
    "    full_ds = full_ds.chunk({\"time\": -1, \"processing_id\": 850})\n",
    "    full_ds = full_ds.rename({rename_dict[metric]: metric})\n",
    "    for scenario, timeframe in analysis_period.items():\n",
    "        compiled_store = f's3://carbonplan-climate-impacts/extreme-heat/v1.0/\"\\\n",
    "                f\"outputs/zarr/daily/{scenario}-WBGT-{metric.split(\"-\")[1]}.zarr'\n",
    "        full_ds.sel(time=timeframe).to_zarr(compiled_store, mode=\"w\", consolidated=True)\n",
    "\n",
    "        ds = xr.open_zarr(compiled_store).chunk({\"gcm\": -1})\n",
    "        summarized = summarize(ds[metric], metric.split(\"-\")[0]).chunk({\"year\": -1})\n",
    "\n",
    "        annual_medians = summarized.sel(year=timeframe).median(dim=\"year\")\n",
    "        ensemble_median = annual_medians.median(dim=\"gcm\")\n",
    "        results = xr.concat(\n",
    "            [\n",
    "                annual_medians,\n",
    "                ensemble_median.expand_dims(dim={\"gcm\": [\"multimodel_median\"]}),\n",
    "            ],\n",
    "            dim=\"gcm\",\n",
    "        ).chunk({\"gcm\": -1})\n",
    "        summary_store = f's3://carbonplan-climate-impacts/extreme-heat/v1.0/outputs/\"\\\n",
    "                f\"zarr/summaries/{scenario}-summaries-WBGT-{metric.split(\"-\")[1]}.zarr'\n",
    "        results = dask.optimize(results)[0]\n",
    "        results.to_zarr(summary_store, mode=\"w\", consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf1d479-f823-400f-b0bc-5970069911de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"s3://carbonplan-extreme-heat/inputs/all_regions_and_cities.json\"\n",
    "with fsspec.open(path) as file:\n",
    "    regions_df = gpd.read_file(file)\n",
    "sample_ds = xr.open_zarr(\n",
    "    \"s3://carbonplan-extreme-heat/temp/wbgt-sun-regions/wbgt-sun-ACCESS-CM2.zarr\"\n",
    ")\n",
    "regions_df = regions_df[\n",
    "    regions_df[\"processing_id\"].isin(sample_ds.processing_id.values)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38d0724-bb93-4cca-9654-e2921982fd44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep_heatwaves(da, metric):\n",
    "    \"\"\"\n",
    "    Load dataset and roll the southern hemisphere data points by 180 days to ensure\n",
    "    that heatwaves are captured correctly in the southern hemisphere.\n",
    "    \"\"\"\n",
    "    southern_hemisphere_ids = regions_df.cx[:, :0].processing_id.values\n",
    "    northern_hemisphere_ids = list(\n",
    "        set(regions_df.processing_id.values) - set(southern_hemisphere_ids)\n",
    "    )\n",
    "    # do hemispheres separately and then concatenate back together to ease computation\n",
    "    selected_da_n = da.sel({\"processing_id\": northern_hemisphere_ids})\n",
    "    selected_da_s = da.sel({\"processing_id\": southern_hemisphere_ids})\n",
    "    selected_da_s = selected_da_s.roll({\"time\": 180})\n",
    "    out_da = xr.concat([selected_da_n, selected_da_s], dim=\"processing_id\")\n",
    "    if metric in [\"wbgt-shade\", \"wbgt-sun\"]:\n",
    "        out_da.attrs[\"units\"] = \"degC\"\n",
    "\n",
    "    return out_da.to_dataset(name=metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacdf26b-8247-48a2-b6b0-5d0df53163ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "Compile all individual GCM datasets into one multimodel dataset that is optimally chunked for heatwave timeseries analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e787afd-70eb-44f1-83c4-5c28d3a6e49c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for metric in [\"wbgt-shade\", \"wbgt-sun\"]:\n",
    "    for scenario, timeframe in analysis_period.items():\n",
    "        compiled_store = (\n",
    "            f\"s3://carbonplan-climate-impacts/extreme-heat/v1.0/outputs/\"\n",
    "            f\"zarr/daily/{scenario}-WBGT-{metric.split('-')[1]}.zarr\"\n",
    "        )\n",
    "        ds = xr.open_zarr(compiled_store)\n",
    "        heat_wave_store = (\n",
    "            f\"s3://carbonplan-scratch/extreme-heat/\"\n",
    "            f\"{metric}-compiled-for-heatwaves-{scenario}.zarr\"\n",
    "        )\n",
    "        out_ds = prep_heatwaves(ds.sel(time=timeframe)[metric], metric)\n",
    "        out_ds.chunk({\"processing_id\": 850}).to_zarr(heat_wave_store, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec4112b-3e31-4b6f-a5ca-08090377a6ae",
   "metadata": {},
   "source": [
    "Create heatwave summaries for each analysis period and multimodel medians of those summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a406e761-0aa0-477d-b5ea-947ab8b9f47b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for metric in [\"wbgt-shade\", \"wbgt-sun\"]:\n",
    "    for scenario, timeframe in analysis_period.items():\n",
    "        if (metric == \"wbgt-shade\") & (scenario == \"historical\"):\n",
    "            print(\"yay\")\n",
    "            continue\n",
    "        print(\"here\")\n",
    "        heat_wave_store = (\n",
    "            f\"s3://carbonplan-scratch/extreme-heat/\"\n",
    "            f\"{metric}-compiled-for-heatwaves-{scenario}.zarr\"\n",
    "        )\n",
    "        da = xr.open_zarr(heat_wave_store)[metric].chunk({\"processing_id\": 10000})\n",
    "        da[\"time\"] = pd.date_range(\n",
    "            da[\"time\"].values[0], da[\"time\"].values[-1], normalize=True\n",
    "        )\n",
    "        period_median = xr.Dataset()\n",
    "\n",
    "        for threshold in [29, 30.5, 32, 35]:\n",
    "            threshold_unit = f\"{threshold} degC\"\n",
    "            period_median[\n",
    "                f\"heatwave-days-over-{threshold}degC\"\n",
    "            ] = xclim.indicators.atmos.heat_wave_index(\n",
    "                da, window=5, freq=\"YS\", thresh=threshold_unit\n",
    "            ).median(\n",
    "                dim=\"time\"\n",
    "            )\n",
    "        period_median.attrs[\"units\"] = \"days_over_threshold\"\n",
    "        ensemble_median = period_median.median(dim=\"gcm\")\n",
    "        results = xr.concat(\n",
    "            [\n",
    "                period_median,\n",
    "                ensemble_median.expand_dims(dim={\"gcm\": [\"multimodel_median\"]}),\n",
    "            ],\n",
    "            dim=\"gcm\",\n",
    "        ).chunk({\"gcm\": 1})\n",
    "        results = dask.optimize(results)[0]\n",
    "        out_file = (\n",
    "            f\"s3://carbonplan-climate-impacts/extreme-heat/v1.0/outputs/zarr/\"\n",
    "            f\"summaries/{scenario}-summaries-heatwaves-WBGT-{metric.split('-')[1]}.zarr\"\n",
    "        )\n",
    "        results.to_zarr(out_file, mode=\"w\", consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b0df4-0ed7-42ad-842d-2f7484b52e25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
